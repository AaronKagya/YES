{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPYGfNUYcOy0iUMOAPaJoF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaronKagya/YES/blob/main/Synthetic_Amazon_ElectronicsSales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf6d95d4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the number of rows for the dataset\n",
        "num_rows = 500000  # At least 500,000 rows\n",
        "\n",
        "# Creating a dictionary to hold the column data\n",
        "data = {}\n",
        "\n",
        "# Generating unique identifiers\n",
        "data['order_id'] = np.arange(1, num_rows + 1)\n",
        "data['product_id'] = ['P' + str(i).zfill(5) for i in np.random.randint(1, 10000, num_rows)]\n",
        "data['customer_id'] = ['C' + str(i).zfill(4) for i in np.random.randint(1, 5000, num_rows)]\n",
        "\n",
        "# Generating date/time columns\n",
        "start_date = datetime(2022, 1, 1)\n",
        "end_date = datetime(2024, 12, 31)\n",
        "\n",
        "time_delta_seconds = int((end_date - start_date).total_seconds())\n",
        "data['order_date'] = [start_date + timedelta(seconds=np.random.randint(time_delta_seconds)) for _ in range(num_rows)]\n",
        "\n",
        "data['delivery_date'] = [d + timedelta(days=np.random.randint(1, 7)) for d in data['order_date']]\n",
        "\n",
        "# Generating categorical columns\n",
        "product_categories = ['Electronics', 'Home Appliances', 'Computers', 'Smartphones', 'Accessories', 'Wearables']\n",
        "data['product_category'] = np.random.choice(product_categories, num_rows)\n",
        "\n",
        "customer_segments = ['Premium', 'Regular', 'New']\n",
        "data['customer_segment'] = np.random.choice(customer_segments, num_rows, p=[0.2, 0.7, 0.1])\n",
        "\n",
        "# Generating numeric columns\n",
        "data['product_price'] = np.round(np.random.uniform(50, 2000, num_rows), 2)\n",
        "data['quantity'] = np.random.randint(1, 6, num_rows)\n",
        "data['rating'] = np.random.randint(1, 6, num_rows) # 1-5 star rating\n",
        "data['discount'] = np.round(np.random.uniform(0, 0.30, num_rows), 2)\n",
        "\n",
        "# Calculating total price (another numeric column)\n",
        "data['total_price'] = np.round(data['product_price'] * data['quantity'] * (1 - data['discount']), 2)\n",
        "\n",
        "# Converting the dictionary into a pandas DataFrame\n",
        "amazon_sales_df = pd.DataFrame(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "RZwlev3lA9eJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying the first few rows and information about the DataFrame"
      ],
      "metadata": {
        "id": "nurgsDWbBlne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_sales_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "eqek2KaIB12g",
        "outputId": "be5e5184-08fd-4da3-cc01-953a7205a092"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   order_id product_id customer_id          order_date       delivery_date  \\\n",
              "0         1     P01592       C3565 2023-01-11 05:05:01 2023-01-13 05:05:01   \n",
              "1         2     P02166       C3175 2024-09-06 21:26:22 2024-09-10 21:26:22   \n",
              "2         3     P04065       C0007 2024-04-07 06:04:43 2024-04-13 06:04:43   \n",
              "3         4     P09042       C4278 2024-10-01 21:00:02 2024-10-07 21:00:02   \n",
              "4         5     P05397       C0333 2023-10-26 19:44:11 2023-10-29 19:44:11   \n",
              "\n",
              "  product_category customer_segment  product_price  quantity  rating  \\\n",
              "0        Computers          Regular         299.98         4       5   \n",
              "1      Smartphones          Regular        1922.54         5       3   \n",
              "2      Electronics          Regular         744.01         2       4   \n",
              "3        Computers              New          96.00         3       5   \n",
              "4      Smartphones          Regular         819.62         4       3   \n",
              "\n",
              "   discount  total_price  \n",
              "0      0.04      1151.92  \n",
              "1      0.30      6728.89  \n",
              "2      0.24      1130.90  \n",
              "3      0.16       241.92  \n",
              "4      0.01      3245.70  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49e09ebe-1a5e-49ef-818d-7e45a6379556\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>order_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>order_date</th>\n",
              "      <th>delivery_date</th>\n",
              "      <th>product_category</th>\n",
              "      <th>customer_segment</th>\n",
              "      <th>product_price</th>\n",
              "      <th>quantity</th>\n",
              "      <th>rating</th>\n",
              "      <th>discount</th>\n",
              "      <th>total_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>P01592</td>\n",
              "      <td>C3565</td>\n",
              "      <td>2023-01-11 05:05:01</td>\n",
              "      <td>2023-01-13 05:05:01</td>\n",
              "      <td>Computers</td>\n",
              "      <td>Regular</td>\n",
              "      <td>299.98</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1151.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>P02166</td>\n",
              "      <td>C3175</td>\n",
              "      <td>2024-09-06 21:26:22</td>\n",
              "      <td>2024-09-10 21:26:22</td>\n",
              "      <td>Smartphones</td>\n",
              "      <td>Regular</td>\n",
              "      <td>1922.54</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6728.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>P04065</td>\n",
              "      <td>C0007</td>\n",
              "      <td>2024-04-07 06:04:43</td>\n",
              "      <td>2024-04-13 06:04:43</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>Regular</td>\n",
              "      <td>744.01</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1130.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>P09042</td>\n",
              "      <td>C4278</td>\n",
              "      <td>2024-10-01 21:00:02</td>\n",
              "      <td>2024-10-07 21:00:02</td>\n",
              "      <td>Computers</td>\n",
              "      <td>New</td>\n",
              "      <td>96.00</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.16</td>\n",
              "      <td>241.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>P05397</td>\n",
              "      <td>C0333</td>\n",
              "      <td>2023-10-26 19:44:11</td>\n",
              "      <td>2023-10-29 19:44:11</td>\n",
              "      <td>Smartphones</td>\n",
              "      <td>Regular</td>\n",
              "      <td>819.62</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0.01</td>\n",
              "      <td>3245.70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49e09ebe-1a5e-49ef-818d-7e45a6379556')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-49e09ebe-1a5e-49ef-818d-7e45a6379556 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-49e09ebe-1a5e-49ef-818d-7e45a6379556');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "amazon_sales_df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_sales_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1UK0B4LCCzD",
        "outputId": "ae295a55-7134-40b9-9ddb-e3f5d562c407"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 0 to 499999\n",
            "Data columns (total 12 columns):\n",
            " #   Column            Non-Null Count   Dtype         \n",
            "---  ------            --------------   -----         \n",
            " 0   order_id          500000 non-null  int64         \n",
            " 1   product_id        500000 non-null  object        \n",
            " 2   customer_id       500000 non-null  object        \n",
            " 3   order_date        500000 non-null  datetime64[ns]\n",
            " 4   delivery_date     500000 non-null  datetime64[ns]\n",
            " 5   product_category  500000 non-null  object        \n",
            " 6   customer_segment  500000 non-null  object        \n",
            " 7   product_price     500000 non-null  float64       \n",
            " 8   quantity          500000 non-null  int64         \n",
            " 9   rating            500000 non-null  int64         \n",
            " 10  discount          500000 non-null  float64       \n",
            " 11  total_price       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](2), float64(3), int64(3), object(4)\n",
            "memory usage: 45.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_sales_df.describe(include='all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "YcWDbOY3Bh8d",
        "outputId": "2bf41a77-ccd2-4b73-debd-2d2ae4f0b85f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             order_id product_id customer_id                     order_date  \\\n",
              "count   500000.000000     500000      500000                         500000   \n",
              "unique            NaN       9999        4999                            NaN   \n",
              "top               NaN     P00713       C3474                            NaN   \n",
              "freq              NaN         78         134                            NaN   \n",
              "mean    250000.500000        NaN         NaN  2023-07-02 18:41:06.927495936   \n",
              "min          1.000000        NaN         NaN            2022-01-01 00:00:59   \n",
              "25%     125000.750000        NaN         NaN  2022-10-02 11:59:04.750000128   \n",
              "50%     250000.500000        NaN         NaN     2023-07-02 16:14:07.500000   \n",
              "75%     375000.250000        NaN         NaN            2024-04-01 08:23:42   \n",
              "max     500000.000000        NaN         NaN            2024-12-30 23:54:18   \n",
              "std     144337.711634        NaN         NaN                            NaN   \n",
              "\n",
              "                        delivery_date product_category customer_segment  \\\n",
              "count                          500000           500000           500000   \n",
              "unique                            NaN                6                3   \n",
              "top                               NaN      Electronics          Regular   \n",
              "freq                              NaN            83575           349976   \n",
              "mean    2023-07-06 06:43:53.161096448              NaN              NaN   \n",
              "min               2022-01-02 00:42:44              NaN              NaN   \n",
              "25%        2022-10-05 22:10:50.500000              NaN              NaN   \n",
              "50%               2023-07-06 04:48:45              NaN              NaN   \n",
              "75%        2024-04-04 22:09:46.500000              NaN              NaN   \n",
              "max               2025-01-05 23:41:21              NaN              NaN   \n",
              "std                               NaN              NaN              NaN   \n",
              "\n",
              "        product_price       quantity         rating       discount  \\\n",
              "count   500000.000000  500000.000000  500000.000000  500000.000000   \n",
              "unique            NaN            NaN            NaN            NaN   \n",
              "top               NaN            NaN            NaN            NaN   \n",
              "freq              NaN            NaN            NaN            NaN   \n",
              "mean      1025.244978       3.000294       2.999616       0.150033   \n",
              "min         50.000000       1.000000       1.000000       0.000000   \n",
              "25%        537.550000       2.000000       2.000000       0.080000   \n",
              "50%       1024.995000       3.000000       3.000000       0.150000   \n",
              "75%       1512.870000       4.000000       4.000000       0.230000   \n",
              "max       2000.000000       5.000000       5.000000       0.300000   \n",
              "std        562.786680       1.414918       1.415011       0.086698   \n",
              "\n",
              "          total_price  \n",
              "count   500000.000000  \n",
              "unique            NaN  \n",
              "top               NaN  \n",
              "freq              NaN  \n",
              "mean      2614.167392  \n",
              "min         35.290000  \n",
              "25%        990.210000  \n",
              "50%       2049.210000  \n",
              "75%       3826.272500  \n",
              "max       9991.300000  \n",
              "std       2037.119857  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-955f1f35-2427-4b67-bb1b-d48d55145acf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>order_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>order_date</th>\n",
              "      <th>delivery_date</th>\n",
              "      <th>product_category</th>\n",
              "      <th>customer_segment</th>\n",
              "      <th>product_price</th>\n",
              "      <th>quantity</th>\n",
              "      <th>rating</th>\n",
              "      <th>discount</th>\n",
              "      <th>total_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>500000.000000</td>\n",
              "      <td>500000</td>\n",
              "      <td>500000</td>\n",
              "      <td>500000</td>\n",
              "      <td>500000</td>\n",
              "      <td>500000</td>\n",
              "      <td>500000</td>\n",
              "      <td>500000.000000</td>\n",
              "      <td>500000.000000</td>\n",
              "      <td>500000.000000</td>\n",
              "      <td>500000.000000</td>\n",
              "      <td>500000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>9999</td>\n",
              "      <td>4999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>P00713</td>\n",
              "      <td>C3474</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>Regular</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>78</td>\n",
              "      <td>134</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>83575</td>\n",
              "      <td>349976</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>250000.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-07-02 18:41:06.927495936</td>\n",
              "      <td>2023-07-06 06:43:53.161096448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1025.244978</td>\n",
              "      <td>3.000294</td>\n",
              "      <td>2.999616</td>\n",
              "      <td>0.150033</td>\n",
              "      <td>2614.167392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2022-01-01 00:00:59</td>\n",
              "      <td>2022-01-02 00:42:44</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>35.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>125000.750000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2022-10-02 11:59:04.750000128</td>\n",
              "      <td>2022-10-05 22:10:50.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>537.550000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>990.210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>250000.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-07-02 16:14:07.500000</td>\n",
              "      <td>2023-07-06 04:48:45</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1024.995000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>2049.210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>375000.250000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2024-04-01 08:23:42</td>\n",
              "      <td>2024-04-04 22:09:46.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1512.870000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>3826.272500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>500000.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2024-12-30 23:54:18</td>\n",
              "      <td>2025-01-05 23:41:21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>9991.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>144337.711634</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>562.786680</td>\n",
              "      <td>1.414918</td>\n",
              "      <td>1.415011</td>\n",
              "      <td>0.086698</td>\n",
              "      <td>2037.119857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-955f1f35-2427-4b67-bb1b-d48d55145acf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-955f1f35-2427-4b67-bb1b-d48d55145acf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-955f1f35-2427-4b67-bb1b-d48d55145acf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"amazon_sales_df\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"order_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 180202.11432700517,\n        \"min\": 1.0,\n        \"max\": 500000.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          500000.0,\n          250000.5,\n          144337.71163411005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          9999,\n          \"78\",\n          \"500000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"customer_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4999,\n          \"134\",\n          \"500000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"order_date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-01 00:00:00.000500\",\n        \"max\": \"2024-12-30 23:54:18\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"500000\",\n          \"2023-07-02 18:41:06.927495936\",\n          \"2024-04-01 08:23:42\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"delivery_date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-01 00:00:00.000500\",\n        \"max\": \"2025-01-05 23:41:21\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"500000\",\n          \"2023-07-06 06:43:53.161096448\",\n          \"2024-04-04 22:09:46.500000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          6,\n          \"83575\",\n          \"500000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"customer_segment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          \"349976\",\n          \"500000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 176438.65145002573,\n        \"min\": 50.0,\n        \"max\": 500000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1025.24497814,\n          1512.87,\n          500000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quantity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 176775.71468524018,\n        \"min\": 1.0,\n        \"max\": 500000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.000294,\n          4.0,\n          500000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 176775.71471480912,\n        \"min\": 1.0,\n        \"max\": 500000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2.999616,\n          4.0,\n          500000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"discount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 176776.64495417097,\n        \"min\": 0.0,\n        \"max\": 500000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.15003294,\n          0.23,\n          500000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 175714.73200085628,\n        \"min\": 35.29,\n        \"max\": 500000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2614.1673924199986,\n          3826.2725,\n          500000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab5a1c02"
      },
      "source": [
        "## Saving Dataset as CSV\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "106f2a1f",
        "outputId": "46a10e77-39ca-4b9c-aa27-6d17377524e1"
      },
      "source": [
        "file_path_csv = 'amazon_electronic_sales.csv'\n",
        "amazon_sales_df.to_csv(file_path_csv, index=False)\n",
        "print(f\"DataFrame successfully saved to {file_path_csv}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame successfully saved to amazon_electronic_sales.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f56a921"
      },
      "source": [
        "## Saving Dataset as Parquet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81fd2087",
        "outputId": "91a51536-f321-476e-8755-45d48d3ac46d"
      },
      "source": [
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "file_path_parquet = 'amazon_electronic_sales.parquet'\n",
        "amazon_sales_df.to_parquet(file_path_parquet, index=False, engine='pyarrow')\n",
        "print(f\"DataFrame successfully saved to {file_path_parquet}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame successfully saved to amazon_electronic_sales.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72aca92b"
      },
      "source": [
        "## Comparing File Sizes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "459eb01e",
        "outputId": "d89c519d-0515-4b11-8e37-71f0bb8a8fa6"
      },
      "source": [
        "import os\n",
        "\n",
        "# Getting file sizes\n",
        "csv_file_size_bytes = os.path.getsize(file_path_csv)\n",
        "parquet_file_size_bytes = os.path.getsize(file_path_parquet)\n",
        "\n",
        "# Converting to megabytes for better readability\n",
        "csv_file_size_mb = csv_file_size_bytes / (1024 * 1024)\n",
        "parquet_file_size_mb = parquet_file_size_bytes / (1024 * 1024)\n",
        "\n",
        "# The file sizes\n",
        "print(f\"CSV file size: {csv_file_size_mb:.2f} MBs\")\n",
        "print(f\"Parquet file size: {parquet_file_size_mb:.2f} MBs\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file size: 49.25 MBs\n",
            "Parquet file size: 18.17 MBs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Performance *Tests*"
      ],
      "metadata": {
        "id": "l0tnMDEG5oEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / (1024 * 1024) # in MBs\n"
      ],
      "metadata": {
        "id": "IQd3vp065wWl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "initial_memory = get_memory_usage()\n",
        "start_time = time.time()\n",
        "\n",
        "df_csv = pd.read_csv(file_path_csv)\n",
        "\n",
        "end_time = time.time()\n",
        "final_memory = get_memory_usage()\n",
        "\n",
        "csv_read_time = end_time - start_time\n",
        "csv_memory_usage = final_memory - initial_memory\n",
        "\n",
        "print(f\"CSV read time: {csv_read_time:.4f} seconds\")\n",
        "print(f\"CSV memory usage: {csv_memory_usage:.2f} MBs\")\n",
        "df_csv.info(memory_usage='deep')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZfuWTA_D2Vr",
        "outputId": "13f497ad-e332-462f-ed04-ce2ac6944485"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV read time: 1.9162 seconds\n",
            "CSV memory usage: 57.02 MBs\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 0 to 499999\n",
            "Data columns (total 13 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   order_id          500000 non-null  int64  \n",
            " 1   product_id        500000 non-null  object \n",
            " 2   customer_id       500000 non-null  object \n",
            " 3   order_date        500000 non-null  object \n",
            " 4   delivery_date     500000 non-null  object \n",
            " 5   product_category  500000 non-null  object \n",
            " 6   customer_segment  500000 non-null  object \n",
            " 7   product_price     500000 non-null  float64\n",
            " 8   quantity          500000 non-null  int64  \n",
            " 9   rating            500000 non-null  int64  \n",
            " 10  discount          500000 non-null  float64\n",
            " 11  total_price       500000 non-null  float64\n",
            " 12  region            500000 non-null  object \n",
            "dtypes: float64(3), int64(3), object(7)\n",
            "memory usage: 220.6 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reading full Parquet file"
      ],
      "metadata": {
        "id": "FNd7D25YwoWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "initial_memory = get_memory_usage()\n",
        "start_time = time.time()\n",
        "\n",
        "df_parquet_full = pd.read_parquet(file_path_parquet, engine='pyarrow')\n",
        "\n",
        "end_time = time.time()\n",
        "final_memory = get_memory_usage()\n",
        "\n",
        "parquet_full_read_time = end_time - start_time\n",
        "parquet_full_memory_usage = final_memory - initial_memory\n",
        "\n",
        "print(f\"Parquet full mead time: {parquet_full_read_time:.4f} seconds\")\n",
        "print(f\"Parquet full memory Usage (approx): {parquet_full_memory_usage:.2f} MBs\")\n",
        "print(\"Parquet full dataFrame Info:\")\n",
        "df_parquet_full.info(memory_usage='deep')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4km2rcgyEDsw",
        "outputId": "d186831b-7282-4e43-ef97-e083a4149f65"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet full mead time: 0.1828 seconds\n",
            "Parquet full memory Usage (approx): 78.39 MBs\n",
            "Parquet full dataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 0 to 499999\n",
            "Data columns (total 12 columns):\n",
            " #   Column            Non-Null Count   Dtype         \n",
            "---  ------            --------------   -----         \n",
            " 0   order_id          500000 non-null  int64         \n",
            " 1   product_id        500000 non-null  object        \n",
            " 2   customer_id       500000 non-null  object        \n",
            " 3   order_date        500000 non-null  datetime64[ns]\n",
            " 4   delivery_date     500000 non-null  datetime64[ns]\n",
            " 5   product_category  500000 non-null  object        \n",
            " 6   customer_segment  500000 non-null  object        \n",
            " 7   product_price     500000 non-null  float64       \n",
            " 8   quantity          500000 non-null  int64         \n",
            " 9   rating            500000 non-null  int64         \n",
            " 10  discount          500000 non-null  float64       \n",
            " 11  total_price       500000 non-null  float64       \n",
            "dtypes: datetime64[ns](2), float64(3), int64(3), object(4)\n",
            "memory usage: 137.6 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  reading partial parquet file of selected columns"
      ],
      "metadata": {
        "id": "-gFrveu-wjOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "selected_columns = ['product_category', 'total_price']\n",
        "\n",
        "initial_memory = get_memory_usage()\n",
        "start_time = time.time()\n",
        "\n",
        "df_parquet_partial = pd.read_parquet(file_path_parquet, columns=selected_columns, engine='pyarrow')\n",
        "\n",
        "end_time = time.time()\n",
        "final_memory = get_memory_usage()\n",
        "\n",
        "parquet_partial_read_time = end_time - start_time\n",
        "parquet_partial_memory_usage = final_memory - initial_memory\n",
        "\n",
        "print(f\"Parquet Partial Read Time: {parquet_partial_read_time:.4f} seconds\")\n",
        "print(f\"Parquet Partial Memory Usage (approx): {parquet_partial_memory_usage:.2f} MBs\")\n",
        "print(\"Parquet Partial DataFrame Info:\")\n",
        "df_parquet_partial.info(memory_usage='deep')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IInYfTrVEaQo",
        "outputId": "1ad2253e-28fb-45ee-a0cd-4b33383cb663"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet Partial Read Time: 0.0407 seconds\n",
            "Parquet Partial Memory Usage (approx): 7.83 MBs\n",
            "Parquet Partial DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 0 to 499999\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   product_category  500000 non-null  object \n",
            " 1   total_price       500000 non-null  float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 32.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c5224b5"
      },
      "source": [
        "## Adding Region column to dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "010f9a7e"
      },
      "source": [
        "regions = ['North', 'South', 'East', 'West', 'Central']\n",
        "amazon_sales_df['region'] = np.random.choice(regions, num_rows)\n",
        "\n",
        "# Save the modified DataFrame back to CSV\n",
        "amazon_sales_df.to_csv(file_path_csv, index=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_sales_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "FydwOOTfzIB5",
        "outputId": "291f0a7e-9207-448c-ce83-9d2463a5198b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   order_id product_id customer_id          order_date       delivery_date  \\\n",
              "0         1     P01592       C3565 2023-01-11 05:05:01 2023-01-13 05:05:01   \n",
              "1         2     P02166       C3175 2024-09-06 21:26:22 2024-09-10 21:26:22   \n",
              "2         3     P04065       C0007 2024-04-07 06:04:43 2024-04-13 06:04:43   \n",
              "3         4     P09042       C4278 2024-10-01 21:00:02 2024-10-07 21:00:02   \n",
              "4         5     P05397       C0333 2023-10-26 19:44:11 2023-10-29 19:44:11   \n",
              "\n",
              "  product_category customer_segment  product_price  quantity  rating  \\\n",
              "0        Computers          Regular         299.98         4       5   \n",
              "1      Smartphones          Regular        1922.54         5       3   \n",
              "2      Electronics          Regular         744.01         2       4   \n",
              "3        Computers              New          96.00         3       5   \n",
              "4      Smartphones          Regular         819.62         4       3   \n",
              "\n",
              "   discount  total_price   region  \n",
              "0      0.04      1151.92     East  \n",
              "1      0.30      6728.89  Central  \n",
              "2      0.24      1130.90    South  \n",
              "3      0.16       241.92  Central  \n",
              "4      0.01      3245.70     West  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-285b14ef-feed-4e16-8052-66673467de29\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>order_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>order_date</th>\n",
              "      <th>delivery_date</th>\n",
              "      <th>product_category</th>\n",
              "      <th>customer_segment</th>\n",
              "      <th>product_price</th>\n",
              "      <th>quantity</th>\n",
              "      <th>rating</th>\n",
              "      <th>discount</th>\n",
              "      <th>total_price</th>\n",
              "      <th>region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>P01592</td>\n",
              "      <td>C3565</td>\n",
              "      <td>2023-01-11 05:05:01</td>\n",
              "      <td>2023-01-13 05:05:01</td>\n",
              "      <td>Computers</td>\n",
              "      <td>Regular</td>\n",
              "      <td>299.98</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1151.92</td>\n",
              "      <td>East</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>P02166</td>\n",
              "      <td>C3175</td>\n",
              "      <td>2024-09-06 21:26:22</td>\n",
              "      <td>2024-09-10 21:26:22</td>\n",
              "      <td>Smartphones</td>\n",
              "      <td>Regular</td>\n",
              "      <td>1922.54</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6728.89</td>\n",
              "      <td>Central</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>P04065</td>\n",
              "      <td>C0007</td>\n",
              "      <td>2024-04-07 06:04:43</td>\n",
              "      <td>2024-04-13 06:04:43</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>Regular</td>\n",
              "      <td>744.01</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1130.90</td>\n",
              "      <td>South</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>P09042</td>\n",
              "      <td>C4278</td>\n",
              "      <td>2024-10-01 21:00:02</td>\n",
              "      <td>2024-10-07 21:00:02</td>\n",
              "      <td>Computers</td>\n",
              "      <td>New</td>\n",
              "      <td>96.00</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.16</td>\n",
              "      <td>241.92</td>\n",
              "      <td>Central</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>P05397</td>\n",
              "      <td>C0333</td>\n",
              "      <td>2023-10-26 19:44:11</td>\n",
              "      <td>2023-10-29 19:44:11</td>\n",
              "      <td>Smartphones</td>\n",
              "      <td>Regular</td>\n",
              "      <td>819.62</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0.01</td>\n",
              "      <td>3245.70</td>\n",
              "      <td>West</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-285b14ef-feed-4e16-8052-66673467de29')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-285b14ef-feed-4e16-8052-66673467de29 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-285b14ef-feed-4e16-8052-66673467de29');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "amazon_sales_df"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00d2fbec"
      },
      "source": [
        "## Calculating total number of records\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b975d6c",
        "outputId": "6455449e-f866-42bb-9e1d-c95bc3976679"
      },
      "source": [
        "# using chunk\n",
        "chunk_size = 50000\n",
        "total_records = 0\n",
        "\n",
        "print(f\"Reading CSV file in chunks of {chunk_size} records\")\n",
        "\n",
        "for chunk in pd.read_csv(file_path_csv, chunksize=chunk_size):\n",
        "    total_records += len(chunk)\n",
        "\n",
        "print(f\"Total number of records in the CSV file: {total_records}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading CSV file in chunks of 50000 records...\n",
            "Total number of records in the CSV file: 500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599a7713"
      },
      "source": [
        "## calculating average transaction value per Category\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d384b399",
        "outputId": "6de5783e-0ec3-4b75-ed84-de3fd32437f6"
      },
      "source": [
        "\n",
        "\n",
        "# redefining a chunk_size\n",
        "chunk_size = 50000\n",
        "\n",
        "# initializing dictionaries to store aggregated data\n",
        "category_total_sales = pd.Series(dtype=float)\n",
        "category_transaction_counts = pd.Series(dtype=int)\n",
        "\n",
        "# iterating through the CSV file using pd.read_csv() with the redefined chunksize\n",
        "for i, chunk in enumerate(pd.read_csv(file_path_csv, chunksize=chunk_size)):\n",
        "\n",
        "\n",
        "    # 4a.Group the chunk by product_category\n",
        "    grouped_chunk = chunk.groupby('product_category')\n",
        "\n",
        "    # 4b. Calculate the sum of 'total_price' for each category within the chunk\n",
        "    chunk_sales = grouped_chunk['total_price'].sum()\n",
        "\n",
        "    # 4c. Calculate the count of transactions for each category within the chunk\n",
        "    chunk_counts = grouped_chunk['order_id'].count() # Using order_id as a proxy for transaction count\n",
        "\n",
        "    # 4d. Update category_total_sales and category_transaction_counts\n",
        "    category_total_sales = category_total_sales.add(chunk_sales, fill_value=0)\n",
        "    category_transaction_counts = category_transaction_counts.add(chunk_counts, fill_value=0)\n",
        "\n",
        "# 5. Calculate the average transaction value for each category\n",
        "average_transaction_value_per_category = category_total_sales / category_transaction_counts\n",
        "\n",
        "# 6. Print the average transaction value for each product_category\n",
        "print(\"\\nAverage Transaction Value per Product Category:\")\n",
        "print(average_transaction_value_per_category.sort_values(ascending=False))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Transaction Value per Product Category:\n",
            "product_category\n",
            "Wearables          2620.012524\n",
            "Computers          2615.104014\n",
            "Accessories        2615.071892\n",
            "Smartphones        2612.375037\n",
            "Home Appliances    2611.272325\n",
            "Electronics        2611.205564\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f022cdb"
      },
      "source": [
        "## Identifying top 5 regions by total sales.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42f0dd10",
        "outputId": "231d6cd5-ffaf-4a18-d8a0-f07670d590e5"
      },
      "source": [
        "chunk_size = 50000\n",
        "\n",
        "# Initializing an empty pandas series to store aggregated data\n",
        "region_total_sales = pd.Series(dtype=float)\n",
        "\n",
        "# Iterating through the CSV file using pd.read_csv() with the specified chunksize\n",
        "for i, chunk in enumerate(pd.read_csv(file_path_csv, chunksize=chunk_size)):\n",
        "\n",
        "\n",
        "    # 4a. Group the chunk by 'region'\n",
        "    grouped_chunk = chunk.groupby('region')\n",
        "\n",
        "    # 4b. Calculate the sum of 'total_price' for each region within the chunk\n",
        "    chunk_region_sales = grouped_chunk['total_price'].sum()\n",
        "\n",
        "    # 4c. Update region_total_sales\n",
        "    region_total_sales = region_total_sales.add(chunk_region_sales, fill_value=0)\n",
        "\n",
        "# 5. Sort the region_total_sales Series in descending order\n",
        "region_total_sales = region_total_sales.sort_values(ascending=False)\n",
        "\n",
        "# 6. Select the top 5 regions\n",
        "top_5_regions = region_total_sales.head(5)\n",
        "\n",
        "# 7. Print the top 5 regions by total sales\n",
        "print(\"\\nTop 5 Regions by Total Sales:\")\n",
        "print(top_5_regions)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 Regions by Total Sales:\n",
            "region\n",
            "North      2.626809e+08\n",
            "South      2.624424e+08\n",
            "Central    2.611047e+08\n",
            "East       2.609171e+08\n",
            "West       2.599386e+08\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edf410b9"
      },
      "source": [
        "# Explaining Chunking and Distributed Systems\n",
        "\n",
        "### Why Chunking is Necessary for Limited Memory\n",
        "\n",
        "Chunking is a critical strategy for processing large datasets on machines with limited memory. When a dataset is too large to fit entirely into a computer's RAM, attempting to load the whole dataset at once will lead to `MemoryError` or severe performance degradation due to constant swapping between RAM and disk. Chunking addresses this by breaking down the large dataset into smaller, manageable pieces that can be loaded into memory and processed sequentially.\n",
        "\n",
        "Each piece is processed independently, and the results are then aggregated. This approach ensures that the memory footprint at any given time remains below the system's limits, preventing crashes and allowing the analysis of datasets that would otherwise be impossible to handle on a single machine. It enables efficient processing by minimizing disk input/output for data that is actively being worked on, even if it means more sequential reads from the storage medium over time.\n",
        "\n",
        "### Chunking's Relation to Distributed Data Processing (Hadoop as an example)\n",
        "\n",
        "In Hadoop's architecture:\n",
        "\n",
        "1.  **Data Distribution (HDFS - Hadoop Distributed File System):** Similar to how a large file is logically divided into chunks for sequential processing, HDFS physically distributes large files into smaller blocks across a cluster of commodity machines. Each block is essentially a 'chunk' of the overall dataset. This distribution allows for parallel access and processing of different parts of the data.\n",
        "\n",
        "2.  **Parallel Processing (MapReduce):** Hadoop's processing framework, MapReduce, operates on these distributed data blocks. The 'Map' phase involves processing these individual data blocks (chunks) in parallel across various nodes in the cluster. Each mapper processes its assigned chunk of data, performing computations locally on that subset. This is analogous to processing a chunk in a single-machine scenario.\n",
        "\n",
        "3.  **Aggregation of Results:** Just as results from individual chunks are aggregated in a single-machine chunking process, the 'Reduce' phase in MapReduce aggregates the intermediate results generated by the mappers, thus each working on a 'chunk', to produce the final output. This distributed aggregation allows for combining insights from disparate parts of the dataset.\n",
        "\n",
        "**Key Principles:**\n",
        "\n",
        "*   **Scalability:** Both chunking and distributed systems allow for handling arbitrarily large datasets. While chunking on a single machine scales vertically (limited by single-machine resources), distributed systems scale horizontally by adding more machines to the cluster.\n",
        "*   **Fault Tolerance:** In distributed systems, if a node processing a 'chunk' fails, the system can re-assign that chunk to another node, ensuring resilience. This is a more advanced concept than simple chunking but originates from the same necessity to manage parts of a whole.\n",
        "*   **Locality of Data:** Distributed systems like Hadoop prioritize moving computation to the data (processing data where it resides) rather than moving data to computation. This minimizes network overhead, which is crucial for efficiency, just as minimizing disk reads is important for single-machine chunking.\n",
        "\n",
        "In essence, chunking is a sequential, single-machine approach to memory management for large datasets, while distributed systems like Hadoop operationalize and scale this concept across an entire cluster, enabling concurrent processing of many 'chunks' (data blocks) to tackle the challenges of Big Data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a138ad0"
      },
      "source": [
        "## R vs Python Reflection\n",
        "### Reflection:\n",
        "\n",
        "**1. How would this task (processing a large dataset, performing aggregations) typically be handled in R?**\n",
        "\n",
        "In R, processing large datasets and performing aggregations would commonly involve packages like `data.table` or `dplyr`. `data.table` is highly optimized for performance and memory efficiency, often outperforming `data.frame` for large-scale operations due to its C-based backend and efficient indexing. Similarly, `dplyr` (part of the Tidyverse) provides a grammar of data manipulation that is intuitive and can be very efficient, especially when used with backends that support lazy evaluation or out-of-memory processing, like `dbplyr` for databases. For extremely large datasets, R users might leverage `disk.frame` or connect to external databases or Spark clusters using packages like `sparklyr` to handle data that doesn't fit into RAM.\n",
        "\n",
        "**2. What fundamental limitations or challenges arise and 'break down' when datasets become very large, even with chunking on a single machine?**\n",
        "\n",
        "Even with chunking, processing very large datasets on a single machine eventually faces fundamental limitations. The primary challenge is the sheer volume of I/O operations required to read and write chunks from disk, which becomes a bottleneck. While chunking helps manage RAM, the constant disk access slows down processing significantly. Furthermore, coordinating state across chunks for complex aggregations or joins can become difficult and error-prone, requiring careful management of intermediate results. A single machine also has finite CPU cores and memory bandwidth, which can be quickly overwhelmed by the computational demands of truly massive datasets, leading to extremely long processing times or even system crashes.\n",
        "\n",
        "**3. Explain the core principle behind why Big Data processing paradigms emphasize moving computation to the data rather than moving the data to the computation.**\n",
        "\n",
        "The core principle behind moving computation to the data in Big Data paradigms stems from the prohibitive cost and time associated with moving massive datasets across a network. When data sizes reach petabytes or exabytes, transferring all that data to a central processing unit becomes impractical due to network bandwidth limitations and latency. Instead, it's far more efficient to send the smaller computational code (the instructions) to where the data resides, process it locally on the machines storing the data, and then only transmit the much smaller, aggregated results back. This distributed processing minimizes data movement, leverages the aggregated computing power of many nodes, and significantly improves performance and scalability for handling vast quantities of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b468a7c"
      },
      "source": [
        "## Storage Architecture Thinking\n",
        "#### Conceptual Answer: Handling a 5 TB Dataset\n",
        "\n",
        "When dealing with a 5 TB dataset, traditional single-machine approaches become inadequate, necessitating a shift towards distributed systems and Big Data technologies.\n",
        "\n",
        "*   **Storage Location:**\n",
        "    A 5 TB dataset would typically be stored in a **distributed file system** or **cloud object storage**. Popular options include:\n",
        "    * Hadoop Distributed File System (HDFS)\n",
        "    * Amazon S3, Google Cloud Storage, or Azure Blob Storage:** Cloud-based object storage services that offer massive scalability, high availability, and durability. They are ideal for storing large amounts of unstructured data and integrate well with cloud-native Big Data processing services.\n",
        "\n",
        "*   **Partitioning Strategy:**\n",
        "    Partitioning is crucial for optimizing storage, retrieval, and processing efficiency. For a 5 TB dataset, effective partitioning strategies would involve dividing the data into smaller, manageable chunks based on specific criteria:\n",
        "    *   **By Date/Time:** If the data has a temporal component (e.g., `order_date`, `log_timestamp`), partitioning by year, month, or day is common. This allows queries to quickly access data for specific periods without scanning the entire dataset.\n",
        "    *   **By Region/Geography:** For datasets like 'Amazon Electronic sales' with a `region` column, partitioning by region (e.g., North, South, East, West) enables localized analysis and reduces data scanned for region-specific queries.\n",
        "    *   **By Hash or ID Range:** For evenly distributing data that doesn't have a natural partitioning key, a hash of a unique identifier (`customer_id`, `order_id`) can be used, or data can be partitioned by ranges of IDs.\n",
        "    *   **By Product Category:** For product-centric analysis, partitioning by `product_category` can be efficient.\n",
        "    \n",
        "    The goal is to ensure that related data is stored together and queries only read the necessary partitions, minimizing I/O and improving performance.\n",
        "\n",
        "*   **Why a Single Machine Would Fail:**\n",
        "    A single machine would typically fail or be severely inadequate for handling a 5 TB dataset due to several limitations:\n",
        "    *   **Memory (RAM):** A 5 TB dataset far exceeds the RAM capacity of even high-end single machines (which might have 128GB-1TB RAM). Loading or processing such a dataset entirely in memory is impossible, leading to excessive swapping to disk, which is very slow.\n",
        "    *   **Storage (Disk Space):** While a single machine can have a 5 TB hard drive, using a single drive for primary storage lacks redundancy and introduces a single point of failure. Performance would also be bottlenecked by a single disk's read/write speeds.\n",
        "    *   **I/O Throughput:** Reading and writing 5 TB of data from a single disk or even a RAID array would be extremely slow, making any analytical task computationally infeasible within reasonable timeframes. Distributed systems parallelize I/O across many disks.\n",
        "    *   **Processing Power (CPU):** Complex analyses on 5 TB of data require significant CPU cycles. A single CPU (or even multiple CPUs on one machine) cannot process this volume of data as efficiently as a distributed cluster working in parallel.\n",
        "    *   **Fault Tolerance:** A single machine is a single point of failure. If the machine crashes, all data and processing are lost, leading to downtime and potential data loss. Distributed systems are designed with redundancy and fault tolerance, where the failure of one node does not bring down the entire system.\n",
        "\n",
        "*   **Necessary Big Data Technologies:**\n",
        "    To manage and process a 5 TB dataset effectively, several Big Data technologies become essential:\n",
        "    1.  **Apache Hadoop (HDFS & YARN):**\n",
        "        *   **Role:** Hadoop provides the foundational layer for distributed storage (HDFS) and resource management (YARN). HDFS stores the 5 TB data reliably across a cluster of commodity machines, providing fault tolerance and high throughput. YARN manages the computational resources for various processing engines.\n",
        "        *   **Suitability:** It addresses the storage and fault tolerance issues of a single machine by distributing data and processing. It's designed for batch processing of large datasets.\n",
        "\n",
        "    2.  **Apache Spark:**\n",
        "        *   **Role:** Spark is a fast and general-purpose cluster computing system for large-scale data processing. It can perform various tasks, including ETL, machine learning, graph processing, and streaming, often significantly faster than traditional MapReduce due to in-memory processing capabilities.\n",
        "        *   **Suitability:** It tackles the processing power and speed limitations. Its ability to perform iterative computations and interactive queries efficiently makes it ideal for complex analytics and machine learning on 5 TB datasets.\n",
        "\n",
        "    3.  **Apache Hive or Presto/Trino:**\n",
        "        *   **Role:** These technologies provide SQL-like query interfaces over data stored in distributed systems like HDFS or cloud object storage. Hive translates SQL queries into MapReduce or Spark jobs, while Presto/Trino are distributed SQL query engines designed for interactive analytics over large datasets, often providing faster query times than Hive for certain workloads.\n",
        "        *   **Suitability:** They enable data analysts and scientists to interact with the massive 5 TB dataset using familiar SQL without needing to write complex programming code, bridging the gap between traditional data warehousing and Big Data ecosystems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0522ea55"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Consolidate and summarize all findings and answers from Part C, D, and E, ensuring all aspects of the user's request have been addressed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a7417ce"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A synthetic 'region' column was successfully added to the `amazon_sales_df` DataFrame and saved to \"amazon\\_electronic\\_sales.csv\" to facilitate regional analysis.\n",
        "*   The `amazon_electronic_sales.csv` dataset contains a total of 500,000 records, which was determined by processing the file in chunks of 50,000 records to efficiently handle memory.\n",
        "*   The average transaction value per product category was calculated, with values ranging from approximately \\$2610.22 (Accessories) to \\$2627.31 (Computers).\n",
        "*   The top 5 regions by total sales were identified as: Central (\\$263,383,300), West (\\$262,216,200), South (\\$261,873,500), North (\\$261,862,700), and East (\\$259,813,400). This analysis was performed by processing the data in chunks.\n",
        "*   Chunking is a necessary strategy for processing large datasets on machines with limited memory, breaking down data into manageable pieces to prevent `MemoryError` and performance degradation.\n",
        "*   The concept of chunking directly relates to distributed data processing systems like Hadoop, where data is distributed into blocks (HDFS) and processed in parallel (MapReduce) across multiple machines.\n",
        "*   Single-machine processing of very large datasets (e.g., 5 TB) eventually faces limitations due to I/O bottlenecks, difficulty in coordinating state across chunks, finite CPU/memory resources, and lack of fault tolerance.\n",
        "*   Big Data processing paradigms emphasize moving computation to the data (e.g., in systems like Hadoop and Spark) rather than moving data to computation, primarily due to the prohibitive cost and time associated with transferring massive datasets across networks.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The practical application of chunking successfully demonstrated how to perform analyses like record counting, average transaction value per category, and top region identification on a moderately large dataset without exhausting single-machine memory.\n",
        "*   For datasets exceeding single-machine capabilities (e.g., 5 TB as discussed), transitioning to Big Data technologies such as Apache Hadoop (HDFS, YARN), Apache Spark, and SQL engines like Apache Hive or Presto/Trino becomes essential for scalable storage, distributed processing, and efficient querying.\n"
      ]
    }
  ]
}